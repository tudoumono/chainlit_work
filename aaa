"""
main.py ï¼ Chainlit + OpenAI ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒª
=========================================
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ãƒ–ãƒ©ã‚¦ã‚¶ã§ãƒãƒ£ãƒƒãƒˆ UI ãŒç«‹ã¡ä¸ŠãŒã‚Šã¾ã™ã€‚
åˆ©ç”¨è€…ã¯ã€ŒGPTâ€‘3.5 / GPTâ€‘4 / GPTâ€‘4oã€ã‚’é€”ä¸­ã§ã‚‚è‡ªç”±ã«åˆ‡ã‚Šæ›¿ãˆã¦å¯¾è©±ã§ãã¾ã™ã€‚

--------------------------------------------------------------------------
ğŸ’¡ â€œã–ã£ãã‚Šå…¨ä½“åƒâ€
--------------------------------------------------------------------------
1. **åˆæœŸåŒ–**      : ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿ã€OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
2. **ãƒ¢ãƒ‡ãƒ«é¸æŠUI**: èµ·å‹•æ™‚ã«ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤ºï¼ˆ`show_model_selection()`ï¼‰
3. **ãƒãƒ£ãƒƒãƒˆå‡¦ç†**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè¨€ã‚’å—ã‘å–ã‚Šã€OpenAI ã¸å•åˆã›ã¦è¿”å´
4. **å±¥æ­´ä¿å­˜**    : ã€Œä¿å­˜ã€ãƒœã‚¿ãƒ³ã§ JSON ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
5. **ãƒ¢ãƒ‡ãƒ«å¤‰æ›´**  : ã„ã¤ã§ã‚‚ã€Œãƒ¢ãƒ‡ãƒ«å¤‰æ›´ã€ãƒœã‚¿ãƒ³ã§å†é¸æŠã§ãã‚‹
6. **åœæ­¢ãƒ»å†é–‹**  : â¹ ã§ç”Ÿæˆä¸­æ–­ã€â–¶ ã§ç¶šãã‹ã‚‰å†é–‹   â˜…ä»Šå›è¿½åŠ ï¼

â€» ãªã‚‹ã¹ã **éã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã‚‚èª­ã‚ã‚‹ã‚ˆã†**ã€å°‚é–€ç”¨èªã‚’å™›ã¿ç •ãã¤ã¤
   ã‚³ãƒ¼ãƒ‰å†…ã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤šã‚ã«å…¥ã‚Œã¦ã„ã¾ã™ã€‚
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os, json
from datetime import datetime
from pathlib import Path

# â–¼ ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ï¼ˆå¤–éƒ¨ï¼‰ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from dotenv import load_dotenv           # .env ã‹ã‚‰å¤‰æ•°ã‚’èª­ã‚€ãŠæ‰‹è»½ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
import chainlit as cl                    # ãƒãƒ£ãƒƒãƒˆUIã‚’è¶…ç°¡å˜ã«ä½œã‚Œã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
from openai import AsyncOpenAI           # OpenAI (GPT ãªã©) ã¨ã‚„ã‚Šå–ã‚Šã™ã‚‹å…¬å¼ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. åˆæœŸè¨­å®š
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()                            # .env ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’å–ã‚Šè¾¼ã‚€
DEBUG_MODE = os.getenv("DEBUG_MODE") == "1"
client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. â€œã©ã®GPTã‚’ä½¿ã†ã‹â€ ã®ãƒªã‚¹ãƒˆå®šç¾©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODELS = [
    ("GPTâ€‘3.5 Turbo", "gpt-3.5-turbo"),   # è»½é‡ãƒ»é«˜é€Ÿãƒ»ä½ä¾¡æ ¼
    ("GPTâ€‘4 Turbo",   "gpt-4-turbo"),     # GPTâ€‘4ãƒ™ãƒ¼ã‚¹ã§é«˜é€Ÿãƒ»å®‰ä¾¡
    ("GPTâ€‘4o",        "gpt-4o"),          # æœ€æ–°ãƒ»ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«
]
get_prefix = lambda: "ğŸ› ï¸ã€ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã€‘\n" if DEBUG_MODE else ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. å…±é€šã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆä¿å­˜ãƒ»ãƒ¢ãƒ‡ãƒ«å¤‰æ›´ãƒ»åœæ­¢ãƒ»å†é–‹ï¼‰ â˜…è¿½åŠ ã‚ã‚Š
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def common_actions(show_resume: bool = False):
    """ç”»é¢ä¸‹ã«ä¸¦ã¹ã‚‹ãƒœã‚¿ãƒ³ã‚’å…±é€šé–¢æ•°ã§ç®¡ç†ï¼ˆDRYï¼‰"""
    base = [
        cl.Action(name="save",          label="ä¿å­˜",        payload={"action": "save"}),
        cl.Action(name="change_model",  label="ãƒ¢ãƒ‡ãƒ«å¤‰æ›´",  payload={"action": "change_model"}),
        cl.Action(name="cancel",        label="â¹ åœæ­¢",      payload={"action": "cancel"}),
                cl.Action(name="shutdown",label="ğŸ”´ ãƒ—ãƒ­ã‚»ã‚¹å®Œå…¨çµ‚äº†",     payload={"action": "shutdown"}),
    ]
    # åœæ­¢å¾Œã«ã ã‘ã€Œâ–¶ ç¶šãã€ãƒœã‚¿ãƒ³ã‚’å‡ºã™
    if show_resume:
        base.append(cl.Action(name="resume", label="â–¶ ç¶šã", payload={"action": "resume"}))
    return base

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. ãƒ¢ãƒ‡ãƒ«é¸æŠUI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def show_model_selection():
    await cl.Message(
        content=f"{get_prefix()}ğŸ§  ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸ã‚“ã§ãã ã•ã„ï¼š",
        actions=[
            cl.Action(name="select_model", label=label, payload={"model": val})
            for label, val in MODELS
        ],
    ).send()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. OpenAI ã¸è³ªå•ã‚’æŠ•ã’ã‚‹é–¢æ•°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def ask_openai(user_message: str,
                     history: list[dict],
                     model: str,
                     temperature: float = 0.7,
                     max_tokens: int = 1024):
    """
    * æ™®æ®µ:   OpenAI ã«å•ã„åˆã‚ã›ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§è¿”ã™
    * ãƒ‡ãƒãƒƒã‚°: ãƒ€ãƒŸãƒ¼æ–‡å­—åˆ—ã‚’è¿”ã™
    """
    if DEBUG_MODE:
        async def fake_stream():
            for chunk in ["ï¼ˆãƒ‡ãƒãƒƒã‚°ï¼‰", "ã“ã‚Œã¯ ", "OpenAI ã‚’ ", "å‘¼ã³å‡ºã—ã¦ ", "ã„ã¾ã›ã‚“ã€‚"]:
                yield type("Chunk", (), {
                    "choices": [type("Choice", (), {
                        "delta": type("Delta", (), {"content": chunk})()
                    })()]
                })()
        return fake_stream()

    messages = history + [{"role": "user", "content": user_message}]
    return await client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens,
        stream=True,
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. Chainlit ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@cl.on_chat_start
async def start():
    await show_model_selection()

@cl.action_callback("change_model")
async def change_model(_):
    await show_model_selection()

@cl.action_callback("select_model")
async def model_selected(action: cl.Action):
    cl.user_session.set("selected_model", action.payload["model"])
    await cl.Message(
        content=f"{get_prefix()}âœ… ãƒ¢ãƒ‡ãƒ«ã€Œ{action.payload['model']}ã€ã‚’é¸æŠã—ã¾ã—ãŸã€‚è³ªå•ã‚’ã©ã†ãï¼",
        actions=common_actions(),
    ).send()

# â˜… åœæ­¢ãƒœã‚¿ãƒ³
@cl.action_callback("cancel")
async def cancel_stream(_):
    cl.user_session.set("cancel_flag", True)
    await cl.Message(content="â¹ ç”Ÿæˆã‚’åœæ­¢ã—ã¾ã™â€¦", actions=common_actions(show_resume=True)).send()

# â˜… å†é–‹ãƒœã‚¿ãƒ³
@cl.action_callback("resume")
async def resume_stream(_):
    last_user_msg = cl.user_session.get("last_user_msg")
    if not last_user_msg:
        await cl.Message(content="å†é–‹ã§ãã‚‹ä¼šè©±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚").send()
        return
    # ã€Œç¶šãã‹ã‚‰ãŠé¡˜ã„ã—ã¾ã™ã€ã‚’è¿½åŠ ã—ã¦å†åº¦ ask_openai
    await on_message(cl.Message(content="ç¶šãã‹ã‚‰ãŠé¡˜ã„ã—ã¾ã™ã€‚", author="user", id="resume"), resume=True)

# â˜… ä¿å­˜ãƒœã‚¿ãƒ³ï¼ˆæ—¢å­˜ï¼‰
@cl.action_callback("save")
async def save_history(_):
    history = cl.user_session.get("chat_history", [])
    if not history:
        return
    out_dir = Path("chatlogs"); out_dir.mkdir(exist_ok=True)
    fp = out_dir / f"session_{datetime.now():%Y%m%d_%H%M%S}.json"
    fp.write_text(json.dumps(history, indent=2, ensure_ascii=False), encoding="utf-8")
    await cl.Message(content="ã“ã®ãƒãƒ£ãƒãƒ«ã§ã®ã‚„ã‚Šå–ã‚Šã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚",
                     elements=[cl.File(name=fp.name, path=str(fp), display="inline")]).send()

# â˜… ãƒ—ãƒ­ã‚»ã‚¹å®Œå…¨çµ‚äº†ãƒœã‚¿ãƒ³
@cl.action_callback("shutdown")
async def shutdown_app(_):
    await cl.Message(content="ğŸ”´ ã‚µãƒ¼ãƒãƒ¼ã‚’çµ‚äº†ã—ã¾ã™â€¦").send()
    await cl.sleep(0.1)           # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡çŒ¶äºˆ
    os._exit(0)                   # å³ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†ï¼ˆSystemExitã‚’ç„¡è¦–ã—ã¦å¼·åˆ¶çµ‚äº†ï¼‰

# ãƒ¡ã‚¤ãƒ³ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒãƒ³ãƒ‰ãƒ©
@cl.on_message
async def on_message(msg: cl.Message, resume: bool = False):
    """
    resume=True ã®ã¨ãã¯ã€Œç¶šãã‹ã‚‰ãŠé¡˜ã„ã—ã¾ã™ã€ãªã©ã®å†…éƒ¨å‘¼ã³å‡ºã—ç”¨
    """
    # -- äº‹å‰ãƒªã‚»ãƒƒãƒˆã¨å±¥æ­´å‡¦ç† ------------------------------
    cl.user_session.set("cancel_flag", False)             # â˜… åœæ­¢ãƒ•ãƒ©ã‚°ã‚’æ¯å›ãƒªã‚»ãƒƒãƒˆ
    history = cl.user_session.get("chat_history", [])
    if not resume:                                        # æœ¬æ¥ã®ãƒ¦ãƒ¼ã‚¶å…¥åŠ›ã ã‘å±¥æ­´ã«æ®‹ã™
        history.append({"role": "user", "content": msg.content})
        cl.user_session.set("last_user_msg", msg.content) # â˜… å†é–‹ç”¨ã«ä¿æŒ

    model = cl.user_session.get("selected_model", "gpt-4o")

    # ç©ºãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œã£ã¦ã€ã‚ã¨ã§é€æ¬¡ update()
    stream_msg = await cl.Message(content="").send()

    try:
        stream = await ask_openai(msg.content, history, model)
        assistant_text = ""

        async for chunk in stream:
            # â˜… åœæ­¢ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸã‚‰ break
            if cl.user_session.get("cancel_flag"):
                await stream.aclose()     # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’é–‰ã˜ã¦å³çµ‚äº†
                break

            delta = chunk.choices[0].delta.content
            if delta:
                assistant_text += delta
                stream_msg.content += delta
                await stream_msg.update()

        if not cl.user_session.get("cancel_flag"):
            history.append({"role": "assistant", "content": assistant_text})

    except Exception as e:
        await cl.Message(content=f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}").send()

    finally:
        cl.user_session.set("chat_history", history)

        # â˜… åœæ­¢ã•ã‚ŒãŸå ´åˆã¯ â–¶ ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤ºã€ãã‚Œä»¥å¤–ã¯é€šå¸¸ãƒœã‚¿ãƒ³
        await cl.Message(
            content="âœ… å¿œç­”å®Œäº†ï¼æ¬¡ã®æ“ä½œã‚’é¸ã‚“ã§ãã ã•ã„ï¼š",
            actions=common_actions(show_resume=cl.user_session.get("cancel_flag"))
        ).send()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“š å‚è€ƒãƒªãƒ³ã‚¯
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"""
ãƒ»Chainlit å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ       : https://docs.chainlit.io
ãƒ»OpenAI Chat API ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹   : https://platform.openai.com/docs/api-reference/chat
ãƒ»python-dotenv ä½¿ã„æ–¹            : https://pypi.org/project/python-dotenv/
"""
