"""
main.py ï¼ Chainlit + OpenAI ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒª
=========================================
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ãƒ–ãƒ©ã‚¦ã‚¶ã§ãƒãƒ£ãƒƒãƒˆ UI ãŒç«‹ã¡ä¸ŠãŒã‚Šã¾ã™ã€‚
åˆ©ç”¨è€…ã¯ã€ŒGPTâ€‘3.5 / GPTâ€‘4 / GPTâ€‘4oâ€‘miniã€ã‚’é€”ä¸­ã§ã‚‚è‡ªç”±ã«åˆ‡ã‚Šæ›¿ãˆã¦å¯¾è©±ã§ãã¾ã™ã€‚

--------------------------------------------------------------------------
ğŸ’¡ â€œã–ã£ãã‚Šå…¨ä½“åƒâ€
--------------------------------------------------------------------------
1. **åˆæœŸåŒ–**      : ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿ã€OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
2. **ãƒ¢ãƒ‡ãƒ«é¸æŠUI**: èµ·å‹•æ™‚ã«ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤ºï¼ˆ`show_model_selection()`ï¼‰
3. **ãƒãƒ£ãƒƒãƒˆå‡¦ç†**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè¨€ã‚’å—ã‘å–ã‚Šã€OpenAI ã¸å•åˆã›ã¦è¿”å´
4. **å±¥æ­´ä¿å­˜**    : ã€Œä¿å­˜ã€ãƒœã‚¿ãƒ³ã§ JSON ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
5. **ãƒ¢ãƒ‡ãƒ«å¤‰æ›´**  : ã„ã¤ã§ã‚‚ã€Œãƒ¢ãƒ‡ãƒ«å¤‰æ›´ã€ãƒœã‚¿ãƒ³ã§å†é¸æŠã§ãã‚‹
6. **ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰**: `DEBUG_MODE=1` ã§ OpenAI ã‚’å‘¼ã°ãšãƒ€ãƒŸãƒ¼å¿œç­”

â€» ãªã‚‹ã¹ã **éã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã‚‚èª­ã‚ã‚‹ã‚ˆã†**ã€å°‚é–€ç”¨èªã‚’å™›ã¿ç •ãã¤ã¤
   ã‚³ãƒ¼ãƒ‰å†…ã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤šã‚ã«å…¥ã‚Œã¦ã„ã¾ã™ã€‚
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os
import json
from datetime import datetime
from pathlib import Path

# â–¼ ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ï¼ˆå¤–éƒ¨ï¼‰ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from dotenv import load_dotenv           # .env ã‹ã‚‰å¤‰æ•°ã‚’èª­ã‚€ãŠæ‰‹è»½ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
import chainlit as cl                    # ãƒãƒ£ãƒƒãƒˆUIã‚’è¶…ç°¡å˜ã«ä½œã‚Œã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
from openai import AsyncOpenAI           # OpenAI (GPT ãªã©) ã¨ã‚„ã‚Šå–ã‚Šã™ã‚‹å…¬å¼ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. åˆæœŸè¨­å®š
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()                            # .env ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’å–ã‚Šè¾¼ã‚€
DEBUG_MODE = os.getenv("DEBUG_MODE") == "1"   # æ–‡å­—åˆ— "1" ãªã‚‰ãƒ‡ãƒãƒƒã‚°
client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
#   â†‘ ChatGPT ã¨åŒã˜ãƒ¢ãƒ‡ãƒ«ã‚’ â€œè‡ªåˆ†ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ â€ ã‹ã‚‰å‘¼ã¶ãŸã‚ã®ã‚¯ãƒ©ã‚¹
#     éåŒæœŸç‰ˆãªã®ã§å¤§é‡ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã«ã‚‚å¼·ã„

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. â€œã©ã®GPTã‚’ä½¿ã†ã‹â€ ã®ãƒªã‚¹ãƒˆå®šç¾©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODELS = [
    ("GPTâ€‘3.5 Turbo",       "gpt-3.5-turbo"),        # è»½é‡ãƒ»é«˜é€Ÿãƒ»ä½ä¾¡æ ¼
    ("GPTâ€‘4 Turbo",         "gpt-4-turbo"),          # GPT-4ãƒ™ãƒ¼ã‚¹ã§é«˜é€Ÿãƒ»å®‰ä¾¡
    ("GPTâ€‘4o",              "gpt-4o"),               # æœ€æ–°ãƒ»ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ï¼ˆéŸ³å£°ãƒ»ç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆï¼‰
]

get_prefix = lambda: "ğŸ› ï¸ã€ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã€‘\n" if DEBUG_MODE else ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ãƒ¢ãƒ‡ãƒ«é¸æŠãƒœã‚¿ãƒ³ã‚’å‡ºã™å…±é€šé–¢æ•°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def show_model_selection():
    """ãƒ¢ãƒ‡ãƒ«é¸æŠãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’1ã‹æ‰€ã§ç”Ÿæˆï¼ˆDRY åŸå‰‡ï¼‰"""
    await cl.Message(
        content=f"{get_prefix()}ğŸ§  ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸ã‚“ã§ãã ã•ã„ï¼š",
        actions=[
            cl.Action(                 # ãƒœã‚¿ãƒ³ UI ã‚’1ã¤ä½œã‚‹
                name="select_model",   # ã‚¯ãƒªãƒƒã‚¯æ™‚ãƒˆãƒªã‚¬ãƒ¼ã¨ãªã‚‹è­˜åˆ¥å­
                label=label,           # ãƒœã‚¿ãƒ³ã«è¡¨ç¤ºã™ã‚‹æ–‡å­—
                payload={"model": val} # â€œã©ã®ãƒ¢ãƒ‡ãƒ«ã‹â€ æƒ…å ±ã‚’æ¸¡ã™ç®±
            )
            for label, val in MODELS
        ],
    ).send()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. OpenAI ã¸è³ªå•ã‚’æŠ•ã’ã‚‹å°ã•ãªé–¢æ•°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def ask_openai(user_message: str,
                     history: list[dict],
                     model: str,
                     temperature: float = 0.7,
                     max_tokens: int = 1024):
    """
    * æ™®æ®µ:   OpenAI ã«å•ã„åˆã‚ã›ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§è¿”ã™
    * ãƒ‡ãƒãƒƒã‚°: ãƒ€ãƒŸãƒ¼æ–‡å­—åˆ—ã‚’ã¡ã³ã¡ã³è¿”ã™ï¼ˆAPIæ–™é‡‘ã‚¼ãƒ­ï¼‰
    """
    # --- ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ€ãƒŸãƒ¼ ---
    if DEBUG_MODE:
        async def fake_stream():
            for chunk in ["ï¼ˆãƒ‡ãƒãƒƒã‚°ï¼‰", "ã“ã‚Œã¯ ", "OpenAI ã‚’ ", "å‘¼ã³å‡ºã—ã¦ ", "ã„ã¾ã›ã‚“ã€‚"]:
                # Chainlit ãŒæœŸå¾…ã™ã‚‹ â€œdelta.contentâ€ æ§‹é€ ã‚’æœ€ä½é™ã‚¨ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                yield type("Chunk", (), {
                    "choices": [type("Choice", (), {
                        "delta": type("Delta", (), {"content": chunk})()
                    })()]
                })()
        return fake_stream()

    # --- æœ¬ç•ª: OpenAI ã¸å•ã„åˆã‚ã› ---
    messages = history + [{"role": "user", "content": user_message}]
    return await client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens,
        stream=True,        # â† â€œæ–‡å­—ãŒç”Ÿæˆã•ã‚Œæ¬¡ç¬¬ã™ãé€ã£ã¦ã­â€ ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    )
    # å‚è€ƒ: https://platform.openai.com/docs/api-reference/chat/create

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ç¾¤ï¼ˆChainlit ãŒè‡ªå‹•çš„ã«å‘¼ã‚“ã§ãã‚Œã‚‹ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@cl.on_chat_start
async def start():
    """ãƒãƒ£ãƒƒãƒˆãƒ«ãƒ¼ãƒ ç”Ÿæˆç›´å¾Œï¼šã¾ãšã¯ãƒ¢ãƒ‡ãƒ«é¸æŠãƒœã‚¿ãƒ³ã‚’æç¤º"""
    await show_model_selection()

@cl.action_callback("change_model")
async def change_model(_action: cl.Action):
    """é€”ä¸­ã§ã‚‚ãƒ¦ãƒ¼ã‚¶ãŒâ€œãƒ¢ãƒ‡ãƒ«å¤‰æ›´â€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ãŸã‚‰å‘¼ã°ã‚Œã‚‹"""
    await show_model_selection()

@cl.action_callback("select_model")
async def model_selected(action: cl.Action):
    """ãƒ¢ãƒ‡ãƒ«ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸã‚‰ãƒ¢ãƒ‡ãƒ«åã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«è¨˜éŒ²"""
    selected = action.payload["model"]            # ãƒœã‚¿ãƒ³å´ã§åŸ‹ã‚ãŸå€¤ã‚’å–å¾—
    cl.user_session.set("selected_model", selected)

    await cl.Message(
        content=f"{get_prefix()}âœ… ãƒ¢ãƒ‡ãƒ«ã€Œ{selected}ã€ã‚’é¸æŠã—ã¾ã—ãŸã€‚è³ªå•ã‚’ã©ã†ãï¼",
        actions=[
            cl.Action(name="save",          label="ä¿å­˜",        payload={"action": "save"}),
            cl.Action(name="change_model",  label="ãƒ¢ãƒ‡ãƒ«å¤‰æ›´",  payload={"action": "change_model"}),
        ],
    ).send()

@cl.action_callback("save")
async def save_history(_action: cl.Action):
    """ã€Œä¿å­˜ã€ãƒœã‚¿ãƒ³ â†’ å±¥æ­´ã‚’ JSON ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    history = cl.user_session.get("chat_history", [])
    if not history:
        return

    out_dir = Path("chatlogs"); out_dir.mkdir(exist_ok=True)
    fp = out_dir / f"session_{datetime.now():%Y%m%d_%H%M%S}.json"
    fp.write_text(json.dumps(history, indent=2, ensure_ascii=False), encoding="utf-8")

    await cl.Message(
        content="ã“ã®ãƒãƒ£ãƒãƒ«ã§ã®ã‚„ã‚Šå–ã‚Šã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚",
        elements=[cl.File(name=fp.name, path=str(fp), display="inline")],
    ).send()

@cl.on_message
async def on_message(msg: cl.Message):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒ†ã‚­ã‚¹ãƒˆã‚’é€ã‚‹ãŸã³ã«å®Ÿè¡Œã•ã‚Œã‚‹ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ä¼šè©±å±¥æ­´ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰å–ã‚Šå‡ºã—ã€ä»Šå›ã®ç™ºè¨€ã‚’è¿½åŠ 
    history = cl.user_session.get("chat_history", [])
    history.append({"role": "user", "content": msg.content})

    model = cl.user_session.get("selected_model", "gpt-4o-mini")

    # ç©ºãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã—ã¦ãŠãã€å¾Œã§é€æ¬¡ update()
    stream_msg = await cl.Message(content="").send()

    try:
        # OpenAI ã‹ã‚‰ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’å—ä¿¡
        stream = await ask_openai(msg.content, history, model)

        assistant_text = ""
        async for chunk in stream:
            delta = chunk.choices[0].delta.content
            if delta:
                assistant_text += delta
                stream_msg.content += delta
                await stream_msg.update()

        history.append({"role": "assistant", "content": assistant_text})

    except Exception as e:
        await cl.Message(content=f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}").send()

    finally:
        # æ›´æ–°ã—ãŸå±¥æ­´ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«æˆ»ã™
        cl.user_session.set("chat_history", history)

        # å¿œç­”å¾Œã€å†åº¦æ“ä½œãƒœã‚¿ãƒ³ã‚’è¡¨ç¤º
        await cl.Message(
            content="âœ… å¿œç­”å®Œäº†ï¼æ¬¡ã®æ“ä½œã‚’é¸ã‚“ã§ãã ã•ã„ï¼šãã®ã¾ã¾ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ã‚Œã°ç¶šã‘ã¦å›ç­”ã—ã¾ã™ã€‚",
            actions=[
                cl.Action(name="save",          label="ä¿å­˜",        payload={"action": "save"}),
                cl.Action(name="change_model",  label="ãƒ¢ãƒ‡ãƒ«å¤‰æ›´",  payload={"action": "change_model"}),
            ],
        ).send()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“š å‚è€ƒãƒªãƒ³ã‚¯
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"""
ãƒ»Chainlit å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ       : https://docs.chainlit.io
ãƒ»OpenAI Chat API ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹   : https://platform.openai.com/docs/api-reference/chat
ãƒ»python-dotenv ä½¿ã„æ–¹            : https://pypi.org/project/python-dotenv/
"""
