"""
models_utils.py - ãƒ¢ãƒ‡ãƒ«é–¢é€£ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
=========================================
OpenAIã®ãƒ¢ãƒ‡ãƒ«å®šç¾©ãƒ»æ¥ç¶šãƒ»å‘¼ã³å‡ºã—é–¢é€£ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
å‹å®‰å…¨æ€§ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ãŸ2.5.5å¯¾å¿œç‰ˆã€‚
"""

import time
import json
import asyncio
import traceback
from typing import List, Dict, Any, Optional, Tuple, AsyncGenerator, Union, Callable

import chainlit as cl
from openai import AsyncOpenAI
from openai.types.chat import ChatCompletionChunk

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ãƒ¢ãƒ‡ãƒ«å®šç¾©ã¨æ¥ç¶š
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆ
MODELS: List[Tuple[str, str]] = [
    # åŠ¹ç‡çš„ãƒ»ä½ã‚³ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
    ("GPTâ€‘3.5 Turbo(è»½é‡ãƒ»é«˜é€Ÿãƒ»ä½ä¾¡æ ¼)", "gpt-3.5-turbo"),         # è»½é‡ãƒ»é«˜é€Ÿãƒ»ä½ä¾¡æ ¼
    ("GPTâ€‘4o mini(GPT-4oã®è»½é‡ç‰ˆ)", "gpt-4o-mini"),             # GPT-4oã®è»½é‡ç‰ˆ
    ("GPTâ€‘4.1 nano(æœ€é€Ÿãƒ»æœ€å®‰ãƒ»100ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³æ–‡è„ˆ)", "gpt-4.1-nano"),           # æœ€é€Ÿãƒ»æœ€å®‰ãƒ»100ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³æ–‡è„ˆ
    
    # æ¨™æº–ãƒ¢ãƒ‡ãƒ«
    ("GPTâ€‘4 Turbo(GPTâ€‘4ãƒ™ãƒ¼ã‚¹ã®é«˜é€Ÿãƒ¢ãƒ‡ãƒ«)", "gpt-4-turbo"),             # GPTâ€‘4ãƒ™ãƒ¼ã‚¹ã®é«˜é€Ÿãƒ¢ãƒ‡ãƒ«
    ("GPTâ€‘4o(ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¯¾å¿œ)", "gpt-4o"),                       # ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¯¾å¿œ
    
    # é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«
    ("GPTâ€‘4.1 mini(100ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³æ–‡è„ˆã€ã‚³ãƒ¼ãƒ‰ç‰¹åŒ–)", "gpt-4.1-mini"),           # 100ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³æ–‡è„ˆã€ã‚³ãƒ¼ãƒ‰ç‰¹åŒ–
    ("GPTâ€‘4.1(æœ€æ–°ãƒ»100ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³æ–‡è„ˆã€ã‚³ãƒ¼ãƒ‰ç‰¹åŒ–)", "gpt-4.1"),                     # æœ€æ–°ãƒ»100ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³æ–‡è„ˆã€ã‚³ãƒ¼ãƒ‰ç‰¹åŒ–
]

# ãƒ¢ãƒ‡ãƒ«æƒ…å ±ãƒãƒƒãƒ—ï¼ˆå‚ç…§ç”¨ï¼‰
MODEL_INFO: Dict[str, Dict[str, Any]] = {
    "gpt-3.5-turbo": {
        "description": "è»½é‡ãƒ»é«˜é€Ÿãƒ»ä½ä¾¡æ ¼ã®ãƒ¢ãƒ‡ãƒ«",
        "context_window": 16385,
        "training_data": "2023å¹´9æœˆã¾ã§",
        "price_per_1k": "$0.0005",
        "strengths": ["é«˜é€Ÿ", "ä½ã‚³ã‚¹ãƒˆ", "ä¸€èˆ¬çš„ãªã‚¿ã‚¹ã‚¯"]
    },
    "gpt-4o-mini": {
        "description": "GPT-4oã®è»½é‡ç‰ˆ",
        "context_window": 128000,
        "training_data": "2023å¹´4æœˆã¾ã§",
        "price_per_1k": "$0.0015",
        "strengths": ["é•·æ–‡å‡¦ç†", "ä½ã‚³ã‚¹ãƒˆ", "åŸºæœ¬çš„ãªç”Ÿæˆã‚¿ã‚¹ã‚¯"]
    },
    "gpt-4.1-nano": {
        "description": "æœ€é€Ÿãƒ»æœ€å®‰ãƒ»100ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³æ–‡è„ˆå¯¾å¿œ",
        "context_window": 1000000,
        "training_data": "2023å¹´12æœˆã¾ã§",
        "price_per_1k": "$0.001",
        "strengths": ["è¶…é•·æ–‡å‡¦ç†", "ä½ã‚³ã‚¹ãƒˆ", "åŸºæœ¬çš„ãªã‚¿ã‚¹ã‚¯"]
    },
    "gpt-4-turbo": {
        "description": "GPTâ€‘4ãƒ™ãƒ¼ã‚¹ã®é«˜é€Ÿãƒ¢ãƒ‡ãƒ«",
        "context_window": 128000,
        "training_data": "2023å¹´4æœˆã¾ã§",
        "price_per_1k": "$0.01",
        "strengths": ["è¤‡é›‘ãªæ¨è«–", "é«˜å“è³ªãªæ–‡ç« ç”Ÿæˆ", "é•·æ–‡å‡¦ç†"]
    },
    "gpt-4o": {
        "description": "ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¯¾å¿œã®ä¸‡èƒ½ãƒ¢ãƒ‡ãƒ«",
        "context_window": 128000,
        "training_data": "2023å¹´12æœˆã¾ã§",
        "price_per_1k": "$0.01",
        "strengths": ["ç”»åƒç†è§£", "é«˜å“è³ªãªæ–‡ç« ç”Ÿæˆ", "è¤‡é›‘ãªæ¨è«–"]
    },
    "gpt-4.1-mini": {
        "description": "100ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³æ–‡è„ˆå¯¾å¿œã€ã‚³ãƒ¼ãƒ‰ç‰¹åŒ–",
        "context_window": 1000000,
        "training_data": "2023å¹´12æœˆã¾ã§",
        "price_per_1k": "$0.015",
        "strengths": ["è¶…é•·æ–‡å‡¦ç†", "ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ", "è¤‡é›‘ãªã‚¿ã‚¹ã‚¯"]
    },
    "gpt-4.1": {
        "description": "æœ€æ–°ãƒ»100ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³æ–‡è„ˆå¯¾å¿œã€ã‚³ãƒ¼ãƒ‰ç‰¹åŒ–",
        "context_window": 1000000,
        "training_data": "2023å¹´12æœˆã¾ã§",
        "price_per_1k": "$0.03",
        "strengths": ["è¶…é•·æ–‡å‡¦ç†", "æœ€é«˜å“è³ªã®æ–‡ç« ç”Ÿæˆ", "æœ€å…ˆç«¯ã®æ¨è«–èƒ½åŠ›"]
    }
}

# ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
def init_openai_client(api_key: Optional[str]) -> Optional[AsyncOpenAI]:
    """OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–"""
    if not api_key:
        print("[WARNING] APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¯åˆæœŸåŒ–ã•ã‚Œã¾ã›ã‚“ã€‚")
        return None
    
    try:
        client = AsyncOpenAI(api_key=api_key)
        print("[INFO] OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒæ­£å¸¸ã«åˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸã€‚")
        return client
    except Exception as e:
        print(f"[ERROR] OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
        traceback.print_exc()
        return None

# ãƒ¢ãƒ‡ãƒ«åã‹ã‚‰ãƒ©ãƒ™ãƒ«ã‚’å–å¾—
def get_model_label(model_id: str) -> str:
    """ãƒ¢ãƒ‡ãƒ«IDã‹ã‚‰ãƒ©ãƒ™ãƒ«ã‚’å–å¾—"""
    for label, model in MODELS:
        if model == model_id:
            return label
    return model_id

# ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®å–å¾—
def get_model_info(model_id: str) -> Dict[str, Any]:
    """ãƒ¢ãƒ‡ãƒ«IDã‹ã‚‰è©³ç´°æƒ…å ±ã‚’å–å¾—"""
    return MODEL_INFO.get(model_id, {
        "description": "æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“",
        "context_window": "ä¸æ˜",
        "training_data": "ä¸æ˜",
        "price_per_1k": "ä¸æ˜",
        "strengths": []
    })

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# OpenAI APIå‘¼ã³å‡ºã—
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def ask_openai(
    client: AsyncOpenAI, 
    user_message: str,
    history: List[Dict[str, str]],
    model: str,
    debug_mode: bool = False,
    temperature: float = 0.7,
    max_tokens: int = 1024,
    system_prompt: Optional[str] = None
) -> AsyncGenerator[ChatCompletionChunk, None]:
    """
    OpenAI APIã«å•ã„åˆã‚ã›ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§è¿”ã™ã‹ã€ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã§ã¯ãƒ€ãƒŸãƒ¼å¿œç­”ã‚’è¿”ã™
    
    Args:
        client: OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        user_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        history: ä¼šè©±å±¥æ­´
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        debug_mode: ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®ãƒ•ãƒ©ã‚°
        temperature: ç”Ÿæˆã®å¤šæ§˜æ€§ï¼ˆ0.0ï½2.0ï¼‰
        max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        system_prompt: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæŒ‡å®šãŒã‚ã‚Œã°ï¼‰
        
    Returns:
        ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¾ãŸã¯ãƒ€ãƒŸãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    """
    start_time = time.time()  # å‡¦ç†æ™‚é–“è¨ˆæ¸¬ã®é–‹å§‹
    
    if debug_mode:
        # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰æ™‚ã¯ãƒ€ãƒŸãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ
        async def fake_stream():
            chunks = ["ï¼ˆãƒ‡ãƒãƒƒã‚°ï¼‰", "ã“ã‚Œã¯ ", "OpenAI ã‚’ ", "å‘¼ã³å‡ºã—ã¦ ", "ã„ã¾ã›ã‚“ã€‚", 
                    f"\n\nãƒ¢ãƒ‡ãƒ«: {model}", f"\næ¸©åº¦: {temperature}"]
            
            for chunk in chunks:
                yield type("Chunk", (), {
                    "choices": [type("Choice", (), {
                        "delta": type("Delta", (), {"content": chunk})()
                    })()],
                    "model": model,
                    "metadata": {
                        "elapsed_ms": 100,
                        "debug": True
                    }
                })()
                await asyncio.sleep(0.5)  # ã‚ˆã‚Šè‡ªç„¶ãªé…å»¶ã‚’è¿½åŠ 
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«è¨˜éŒ²
        print(f"[DEBUG] ãƒ¢ãƒ‡ãƒ«: {model}, æ¸©åº¦: {temperature}, ãƒˆãƒ¼ã‚¯ãƒ³: {max_tokens}")
        print(f"[DEBUG] ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {user_message[:50]}...")
        
        return fake_stream()

    try:
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æº–å‚™
        messages = []
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°è¿½åŠ 
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        # å±¥æ­´ã‚’è¿½åŠ 
        messages.extend(history)
        
        # æœ€æ–°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        messages.append({"role": "user", "content": user_message})
        
        # APIã‚³ãƒ¼ãƒ«æ™‚é–“ã¨ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
        print(f"[INFO] OpenAI APIå‘¼ã³å‡ºã—: ãƒ¢ãƒ‡ãƒ«={model}, æ¸©åº¦={temperature}, ãƒˆãƒ¼ã‚¯ãƒ³={max_tokens}")
        
        response_stream = await client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            stream=True,
        )
        
        # æ¸¬å®šãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ 
        elapsed_ms = round((time.time() - start_time) * 1000)
        
        class EnhancedStream:
            """
            æ‹¡å¼µã•ã‚ŒãŸã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¯ãƒ©ã‚¹ - ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            """
            def __init__(self, original_stream, metadata):
                self.original_stream = original_stream
                self.metadata = metadata
            
            def __aiter__(self):
                return self
            
            async def __anext__(self):
                try:
                    chunk = await self.original_stream.__anext__()
                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ£ãƒ³ã‚¯ã«è¿½åŠ 
                    setattr(chunk, 'metadata', self.metadata)
                    return chunk
                except StopAsyncIteration:
                    raise StopAsyncIteration
            
            async def aclose(self):
                await self.original_stream.aclose()
        
        # æ‹¡å¼µã—ãŸã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’è¿”ã™
        metadata = {
            "elapsed_ms": elapsed_ms,
            "model": model,
            "temperature": temperature,
            "start_time": start_time,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        return EnhancedStream(response_stream, metadata)
    
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è©³ç´°ã«è¨˜éŒ²
        error_message = f"OpenAI APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {str(e)}"
        print(f"[ERROR] {error_message}")
        traceback.print_exc()
        
        # ã‚¨ãƒ©ãƒ¼å†…å®¹ã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«è©³ç´°è¡¨ç¤º
        if hasattr(e, 'response'):
            try:
                error_data = e.response.json()
                print(f"[ERROR] APIå¿œç­”è©³ç´°: {json.dumps(error_data, indent=2, ensure_ascii=False)}")
            except:
                print(f"[ERROR] APIå¿œç­”ã®è©³ç´°å–å¾—ã«å¤±æ•—")
        
        # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã¯ãƒ€ãƒŸãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã§ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™
        async def error_stream():
            error_chunks = [
                "âš ï¸ APIã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: ",
                f"{str(e)}",
                "\n\nåˆ¥ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™ã‹ã€è¨­å®šã‚’å¤‰æ›´ã—ã¦ã¿ã¦ãã ã•ã„ã€‚"
            ]
            
            for chunk in error_chunks:
                yield type("Chunk", (), {
                    "choices": [type("Choice", (), {
                        "delta": type("Delta", (), {"content": chunk})()
                    })()],
                    "model": model,
                    "metadata": {
                        "elapsed_ms": round((time.time() - start_time) * 1000),
                        "error": True,
                        "error_type": type(e).__name__
                    }
                })()
                await asyncio.sleep(0.2)
                
        return error_stream()

# ãƒ¢ãƒ‡ãƒ«é¸æŠUIè¡¨ç¤ºï¼ˆæƒ…å ±ä»˜ãï¼‰
async def show_model_selection_with_info() -> None:
    """ãƒ¢ãƒ‡ãƒ«é¸æŠUIã‚’è©³ç´°æƒ…å ±ä»˜ãã§è¡¨ç¤º"""
    # ç¾åœ¨é¸æŠã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
    current_model = cl.user_session.get_typed("selected_model", str, "gpt-4o")
    
    message_content = (
        "## ğŸ§  ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„\n\n"
        "ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦æ©Ÿèƒ½ã‚„ä¾¡æ ¼ãŒç•°ãªã‚Šã¾ã™ã€‚ç›®çš„ã«åˆã‚ã›ã¦é¸æŠã—ã¦ãã ã•ã„ã€‚\n\n"
    )
    
    # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®è©³ç´°æƒ…å ±ã‚’è¡¨ç¤º
    for label, model_id in MODELS:
        info = get_model_info(model_id)
        selected_mark = " âœ“" if model_id == current_model else ""
        
        message_content += (
            f"### {label}{selected_mark}\n"
            f"- {info['description']}\n"
            f"- æ–‡è„ˆçª“: {info['context_window']} ãƒˆãƒ¼ã‚¯ãƒ³\n"
            f"- ä¾¡æ ¼: {info['price_per_1k']}/1,000ãƒˆãƒ¼ã‚¯ãƒ³\n\n"
        )
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®è¿½åŠ ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å›é¿ï¼‰
    timestamp = int(time.time())
    
    await cl.Message(
        content=message_content,
        actions=[
            cl.Action(
                name="select_model", 
                label=label.split("(")[0], 
                description=f"{model} - {get_model_info(model)['description']}",
                payload={"model": model, "timestamp": timestamp}
            )
            for label, model in MODELS
        ],
        tooltip="å„ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´ã¨æœ€é©ãªç”¨é€”"
    ).send()