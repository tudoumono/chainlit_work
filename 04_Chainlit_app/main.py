"""
ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«
ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã§ã™
"""

import os
import logging
from typing import Dict, Any, List, Optional
from debug_helper import log_object_info
import traceback

import chainlit as cl
from chainlit.input_widget import Select
from openai.types.chat import ChatCompletionChunk

from config import CHAT_HISTORY_FILE, CHAT_HISTORY_DIR, MODELS, DEFAULT_MODEL, ACCEPTED_FILE_TYPES, MAX_FILES, MAX_FILE_SIZE_MB
from models import set_current_model, get_current_model, get_model_details
from chat_handler import process_message, add_to_chat_history, save_chat_history
from file_handler import process_uploaded_file, get_all_uploaded_files
from utils import create_model_display_list, save_thread

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®šã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from log_helper import setup_logging, get_logger

# ãƒ­ã‚®ãƒ³ã‚°ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã—ã¦ã€ç¾åœ¨ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—
log_file = setup_logging()

# ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ç”¨ã®ãƒ­ã‚¬ãƒ¼ã‚’å–å¾—
logger = get_logger(__name__)

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•ãƒ­ã‚°
logger.info("ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’èµ·å‹•ã—ã¾ã—ãŸã€‚ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: %s", log_file)

# ç¾åœ¨ã®ãƒãƒ£ãƒƒãƒˆè¨­å®š
chat_settings: Dict[str, Any] = {
    "model": DEFAULT_MODEL,
    "temperature": 0.7,
    "system_prompt": "ã‚ãªãŸã¯è¦ªåˆ‡ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ç°¡æ½”ã‹ã¤æ­£ç¢ºãªå›ç­”ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚"
}


async def show_processing_indicator(text: str = "å‡¦ç†ä¸­...") -> None:
    """
    å‡¦ç†ä¸­ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚
    
    Args:
        text (str): è¡¨ç¤ºã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
    """
    await cl.Message(
        content=text,
        author="ã‚·ã‚¹ãƒ†ãƒ ",
        metadata={"is_processing": True}
    ).send()


@cl.on_chat_start
async def on_chat_start():
    """
    ãƒãƒ£ãƒƒãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹æ™‚ã®å‡¦ç†ã§ã™ã€‚
    """
    logger.info("æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒé–‹å§‹ã•ã‚Œã¾ã—ãŸ")
    
    # ãƒãƒ£ãƒƒãƒˆã®ä¿å­˜å…ˆã‚’è¡¨ç¤º
    await cl.Message(
        content=f"ğŸ“ ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ä¿å­˜å…ˆ: {CHAT_HISTORY_DIR}",
        author="ã‚·ã‚¹ãƒ†ãƒ "
    ).send()
    
    logger.info(f"ãƒãƒ£ãƒƒãƒˆå±¥æ­´ä¿å­˜å…ˆã‚’é€šçŸ¥: {CHAT_HISTORY_DIR}")
    
    try:
        # ä½¿ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’è¡¨ç¤º
        model_values = [model[1] for model in MODELS]  # APIãƒ¢ãƒ‡ãƒ«å
        model_display = [model[0] for model in MODELS]  # è¡¨ç¤ºå
        
        # ãƒ¢ãƒ‡ãƒ«é¸æŠã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã‚’ä½œæˆ
        logger.debug(f"ãƒ¢ãƒ‡ãƒ«é¸æŠã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã‚’ä½œæˆ: {model_values}")
        settings = await cl.ChatSettings(
            [
                Select(
                    id="model",
                    label="OpenAIãƒ¢ãƒ‡ãƒ«",
                    values=model_values,
                    description="ä½¿ç”¨ã™ã‚‹OpenAIãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„",
                    initial_index=model_values.index(DEFAULT_MODEL) if DEFAULT_MODEL in model_values else 0
                )
            ]
        ).send()
        
        # åˆæœŸãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š
        if "model" in settings:
            chat_settings["model"] = settings["model"]
            model_info = await set_current_model(settings["model"])
            
            logger.info(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚Šãƒ¢ãƒ‡ãƒ«ãŒé¸æŠã•ã‚Œã¾ã—ãŸ: {settings['model']}")
            
            # ãƒ¢ãƒ‡ãƒ«è¨­å®šã®é€šçŸ¥
            await cl.Message(
                content=f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ã‚’**{model_info['display_name']}**ã«è¨­å®šã—ã¾ã—ãŸã€‚",
                author="ã‚·ã‚¹ãƒ†ãƒ "
            ).send()
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š
            model_info = await set_current_model(DEFAULT_MODEL)
            logger.info(f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š: {DEFAULT_MODEL}")
            
            await cl.Message(
                content=f"ğŸ“ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«**{model_info['display_name']}**ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚",
                author="ã‚·ã‚¹ãƒ†ãƒ "
            ).send()
    except Exception as e:
        error_msg = f"ãƒ¢ãƒ‡ãƒ«è¨­å®šä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
        logger.error(error_msg)
        logger.error(f"è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±: {traceback.format_exc()}")
        
        await cl.Message(
            content=f"âš ï¸ ãƒ¢ãƒ‡ãƒ«è¨­å®šä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚",
            author="ã‚·ã‚¹ãƒ†ãƒ "
        ).send()
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æƒ…å ±ã‚’è¡¨ç¤º
    try:
        file_types = ", ".join([f"*{ext}" for ext in [".txt", ".log"]])
        await cl.Message(
            content=f"ğŸ“„ **ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**\nã‚µãƒãƒ¼ãƒˆã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file_types}\næœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {MAX_FILES}ãƒ•ã‚¡ã‚¤ãƒ«\næœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {MAX_FILE_SIZE_MB}MB",
            author="ã‚·ã‚¹ãƒ†ãƒ "
        ).send()
        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æƒ…å ±ã‚’é€šçŸ¥: {file_types}, æœ€å¤§ {MAX_FILES}ãƒ•ã‚¡ã‚¤ãƒ«, æœ€å¤§ {MAX_FILE_SIZE_MB}MB")
    except Exception as e:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æƒ…å ±è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logger.error(f"è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±: {traceback.format_exc()}")


@cl.on_settings_update
async def on_settings_update(settings):
    """
    ãƒãƒ£ãƒƒãƒˆè¨­å®šãŒæ›´æ–°ã•ã‚ŒãŸã¨ãã®å‡¦ç†ã§ã™ã€‚
    
    Args:
        settings: æ›´æ–°ã•ã‚ŒãŸè¨­å®šå€¤
    """
    global chat_settings
    
    # ãƒ¢ãƒ‡ãƒ«ãŒå¤‰æ›´ã•ã‚ŒãŸå ´åˆ
    if "model" in settings and settings["model"] != chat_settings["model"]:
        old_model = chat_settings["model"]
        new_model = settings["model"]
        
        # æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š
        model_info = await set_current_model(new_model)
        chat_settings["model"] = new_model
        
        # ãƒ¢ãƒ‡ãƒ«å¤‰æ›´ã®é€šçŸ¥
        await cl.Message(
            content=f"ğŸ”„ ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›´ã—ã¾ã—ãŸ: **{get_model_details(old_model)['display_name']}** â†’ **{model_info['display_name']}**",
            author="ã‚·ã‚¹ãƒ†ãƒ "
        ).send()


@cl.on_message
async def on_message(message: cl.Message):
    """
    ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å—ä¿¡ã—ãŸã¨ãã®ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ã€‚
    
    Args:
        message (cl.Message): å—ä¿¡ã—ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    """
    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—
    model = get_current_model()
    model_info = get_model_details(model)
    
    # å‡¦ç†ä¸­ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    await show_processing_indicator()
    
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†
    response = await process_message(
        message=message.content,
        files=message.elements,
        system_prompt=chat_settings.get("system_prompt", "ã‚ãªãŸã¯è¦ªåˆ‡ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ç°¡æ½”ã‹ã¤æ­£ç¢ºãªå›ç­”ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚"),
        temperature=chat_settings.get("temperature", 0.7)
    )
    
    # å‡¦ç†ã«å¤±æ•—ã—ãŸå ´åˆ
    if not response["success"]:
        await cl.Message(
            content=f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {response.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}",
            author="ã‚·ã‚¹ãƒ†ãƒ "
        ).send()
        return
    
    # AIã®å¿œç­”ç”¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
    ai_message = cl.Message(content="", author=model_info["display_name"])
    await ai_message.send()
    
    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãŒæˆåŠŸã—ãŸå ´åˆ
    if response.get("stream"):
        # Responses APIã§ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†
        response_stream = response["stream"]
        collected_response = ""
        
        try:
            # ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å‡¦ç†
            async for chunk in response_stream:
                # ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                logger.debug(f"å—ä¿¡ã—ãŸã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—: {type(chunk).__name__}")
                
                # æ–°ã—ã„OpenAI Responsesã‚¿ã‚¤ãƒ—ã®å‡¦ç†
                if hasattr(chunk, 'type'):
                    # ResponseCreatedEventã®å‡¦ç†
                    if chunk.type == 'response_created':
                        # åˆæœŸåŒ–ã‚¤ãƒ™ãƒ³ãƒˆãªã®ã§ä½•ã‚‚ã—ãªã„
                        logger.debug("Response created event received")
                        continue
                        
                    # TextDeltaEventã®å‡¦ç†
                    elif chunk.type == 'text_delta':
                        if hasattr(chunk, 'delta'):
                            text_content = chunk.delta
                            collected_response += text_content
                            # Chainlit 2.5.5ã§ã®ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆæ–¹æ³•
                            ai_message.content = collected_response
                            await ai_message.update()
                            
                    # TextStopEventã®å‡¦ç†
                    elif chunk.type == 'text_stop':
                        logger.debug("Text stop event received")
                    
                    # ãã®ä»–ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—
                    else:
                        logger.debug(f"Unknown event type: {chunk.type}")
                
                # æ—§å½¢å¼ã®å‡¦ç† (äº’æ›æ€§ã®ãŸã‚)
                elif hasattr(chunk, 'choices') and chunk.choices and len(chunk.choices) > 0:
                    if hasattr(chunk.choices[0], 'delta') and hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content:
                        text_content = chunk.choices[0].delta.content
                        collected_response += text_content
                        ai_message.content = collected_response
                        await ai_message.update()
                
                # ä»–ã®ã‚¿ã‚¤ãƒ—ã®ãƒãƒ£ãƒ³ã‚¯ï¼ˆæœªçŸ¥ã®å½¢å¼ï¼‰
                else:
                    logger.debug(f"æœªçŸ¥ã®ãƒãƒ£ãƒ³ã‚¯å½¢å¼: {chunk}")
                        
        except Exception as e:
            logger.error(f"ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            logger.error(traceback.format_exc())
            ai_message.content = f"{collected_response}\n\n[ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ]"
            await ai_message.update()
            return
        
        # å¿œç­”ãŒçµ‚äº†ã—ãŸã‚‰ã€ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
        add_to_chat_history("assistant", collected_response)
        save_chat_history()
    else:
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãŒãªã„å ´åˆã€é€šå¸¸ã®å¿œç­”ã‚’è¡¨ç¤º
        response_text = response.get("message", "å¿œç­”ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        ai_message.content = response_text
        await ai_message.update()
        
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
        add_to_chat_history("assistant", response_text)
        save_chat_history()


@cl.on_chat_end
async def on_chat_end():
    """
    ãƒãƒ£ãƒƒãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã®å‡¦ç†ã§ã™ã€‚
    """
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä¿å­˜
    save_chat_history()
    
    # ã‚¹ãƒ¬ãƒƒãƒ‰æƒ…å ±ã‚’å–å¾—
    thread_data = cl.user_session.get("thread", {})
    if thread_data:
        # ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ä¿å­˜
        filepath = save_thread(thread_data)
        logger.info(f"ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")


if __name__ == "__main__":
    # ç›´æ¥å®Ÿè¡Œã•ã‚ŒãŸå ´åˆ
    print(f"ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’èµ·å‹•ã—ã¾ã™ã€‚Chainlitã‚’ä½¿ç”¨ã—ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„: chainlit run main.py")
    print(f"ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ä¿å­˜å…ˆ: {CHAT_HISTORY_DIR}")